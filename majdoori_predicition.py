# -*- coding: utf-8 -*-
"""Majdoori_predicition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IU-1a9AlVA8crBT85noT0asjmH8DJVFl

## Salary Prediction

### 1. Understand the Problem
"""

# Here we are predicting the salary from the uncleaned_salaryData.
# Total 6 features/ attributes are there
# The first 5 columns are input features/independent variavbles
# The last column is Salary ie. Output feature/dependent variable

# Its a regression problem - Accordingly we will be using ML algorithm later

"""### 2. Collection the Data"""

# Program to mount the Google drive
from google.colab import drive
drive.mount('/content/drive')

# Importing pandas library to load our csv dataset file
# It allows us to read, write, check the dataset

import pandas as pd

filePath = "/content/drive/MyDrive/Courses/ML_BI/Uncleaned_SalaryData.csv"
df = pd.read_csv(filePath)
# print(type(df))
df.head()

"""### 3. Data Cleaning (Data understanding and Cleaning process)"""

# Data Understanding Steps

# It shows first 5 rows of the data along with the headers
df.head()   # By default returns 5 rows
df.head(3)  # limit the rows to 3

# Shows the last 5 rows of the dataset
df.tail()  # Bydefault 5 rows will be displayed
# df.tail(2)   # We can mention the row values to limit the entires

df.columns   # Shows the column names

df.shape   # Returns the shape of the dataset (Rows, Column)

df.info()  # Gives the datatype details of the columns

df.describe()  # it gives the statistics of numeric columns

df['Job Title'].unique()

df["Gender"].unique()

df['Education Level'].unique()

## Data Cleaning

# Checking the null value

df.isnull().sum()

# The below syntax shows the null values row wise (axis=1) to represent rows
# NaN - Stands for Not a Number (It will be displayed for empty values)

df[pd.isnull(df).any(axis=1)]
# df[pd.isnull(df)]

## How to handle the missing values

# Mean
# Works well for normally distributed numeric columns

# Median
# Works well for Skewed numeric column

# Mode
# For Categorical Data

"""#data Visualization"""

import seaborn as sns
import matplotlib.pyplot as plt

# Platting the  box plot (Use it for numeric/ non numeric Data )
sns.boxplot(df['Age'])
plt.title("boxplot of age")
plt.show()

sns.boxplot(df['Salary'])
plt.title("Boxplot of Salary")
plt.show()

sns.boxplot(df['Years of Experience'])
plt.title("Boxplot of years of experience")
plt.show()

sns.boxplot(data = df, x = 'Gender',y = 'Salary')
plt.xlabel("Gender of Employee")
plt.ylabel("Salary of Employee")
plt.title("BoxPlot of Gender V/S Salary")
plt.show()

sns.boxplot(data = df, x = 'Education Level',y = 'Salary')
plt.xlabel("Education Level of Employee")
plt.ylabel("Salary of Employee")
plt.title("BoxPlot of Education Level V/S Salary")
plt.show()

sns.scatterplot(data = df, x = 'Years of Experience',y = 'Salary')
plt.xlabel("Years of Experience Employee")
plt.ylabel("Salary of Employee")
plt.title("ScatterPlot of Years of Experience V/S Salary")
plt.show()

sns.scatterplot(data = df,x = "Age",y = "Salary")
plt.xlabel("Age of the Employee")
plt.ylabel("Salary of the Employee")
plt.title("ScatterPlot of Age v/s Salary")
plt.show()

plt.figure(figsize=(8,10))
sns.histplot(df['Salary'],color='yellow')
plt.title("Histogram of the salary")
plt.show()

plt.figure(figsize=(8,10))
sns.histplot(df['Years of Experience'],color='red')
plt.title("Histogram of the years of experience")
plt.show()

plt.figure(figsize=(8,10))
sns.histplot(df['Age'],color='green')
plt.title("Histogram of the age")
plt.show()

# Above we have plotted boxplot for numeric column
# This helps us to identify outliers (Extreme Values)

sns.histplot(df['Age'])
plt.show()

sns.histplot(df['Years of Experience'])
plt.show()

df['Gender'] = df['Gender'].str.strip()

df['Gender'] = df['Gender'].str.lower()

r = {
    "femle":'female'
}

df['Gender'] = df['Gender'].replace(r)

# CountPlot for Categorical Data

# sns.histplot(df['Gender'])
# plt.show()
import seaborn as sns
import matplotlib.pyplot as plt
sns.countplot(df['Gender'])
plt.show()

df['Education Level'] = df['Education Level'].str.strip()

df['Education Level'] = df['Education Level'].str.lower()

rp = {
    "bachelors": "bachelor's",
    "bachelor": "bachelor's",
    "masters": "master's",
    "phd": "PhD"
}

df['Education Level'] = df['Education Level'].replace(rp)

plt.figure(figsize=(8,10))
sns.countplot(df['Education Level'],color='orange')
plt.title("countplot of the education level")
plt.show()

sns.countplot(df['Education Level'])
plt.show()

#label encoding
from sklearn.preprocessing import LabelEncoder

l = LabelEncoder()

cat_cols = df.select_dtypes(include="object").columns
label_encoder = {}
for col in cat_cols:
  l = LabelEncoder()
  df[col] = l.fit_transform(df[col])
  label_encoder[col] = l

df.info()

numeric_df = df.select_dtypes(include=['number'])
sns.heatmap(numeric_df.corr(), cmap = 'coolwarm')
plt.show()

sns.heatmap(numeric_df.corr(), annot = True, cmap = 'coolwarm')
plt.show()

#Different themes
sns.heatmap(numeric_df.corr(), annot = True, cmap = 'viridis')
plt.show()

# Removing the Null Values

df.isnull().sum()

df = df.dropna()

df.isnull().sum()

df.shape

df.duplicated().sum()

print(df.duplicated().sum())

# Command to drop the duplicate values

# df.drop_duplicates(inplace=True)

df.drop_duplicates(inplace=True)

print(df.drop_duplicates(inplace=True))

"""#session_3"""

df['Age'].unique()

df = df[(df['Age'] >= 18) & (df['Age'] <= 60)]

plt.figure(figsize = (8,10))
sns.histplot(df['Age'],color = 'skyblue')

plt.title('Histogram of the age of the employees above 18 & under 60')
plt.xlabel('Age of the Employee')
plt.ylabel('Number of the Employees')
plt.show()

df.skew(numeric_only=True)
#no messing with target variable -->salary
#only -1 to +1 is allowed, otherwise logtransform and .... is used
#negative skew --> till 0
#Positive skew --> (0,+ve n>0)

"""##Model Selection"""

#split x & y
x = df.drop('Salary',axis=1) #<-- features
y = df['Salary'] # <-- target variable

x.columns

#splitting data into Training & testing
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2, random_state=42) # 0.2 = 20% of the data

#linear Regression
from sklearn.linear_model import LinearRegression

lr = LinearRegression()
lr.fit(x_train,y_train)

y_pred_lr = lr.predict(x_test)

from sklearn.metrics import r2_score
r2 = r2_score(y_test,y_pred_lr)

print("r2_score: ",r2)

"""##Decision tree regressor"""

from sklearn.tree import DecisionTreeRegressor
dt = DecisionTreeRegressor()
dt.fit(x_train,y_train)

y_pred_dt = dt.predict(x_test)
r2_dtr = r2_score(y_test, y_pred_dt)
print("r2 score is: ",r2_dtr)

from sklearn.ensemble import RandomForestRegressor
rfr = RandomForestRegressor()
rfr.fit(x_train,y_train)

y_pred_rfr = rfr.predict(x_test)
r2_rfr = r2_score(y_test,y_pred_lr)
print("r2 score is: ",r2_rfr)

"""##XGboost"""

from xgboost import XGBRegressor
xg = XGBRegressor()
xg.fit(x_train,y_train)

y_pred_xg = xg.predict(x_test)
r2_xg = r2_score(y_test,y_pred_xg)
print("r2_score: ",r2_xg)

from sklearn.metrics import mean_absolute_error
m = mean_absolute_error(y_test,y_pred_rfr)
print("r2_score: ",m)

imp = pd.Series(rfr.feature_importances_, index = x_train.columns)
imp.sort_values().plot(kind='barh',title='Random forest regressor feature importance')
plt.show()

import joblib

joblib.dump(rfr,"Salary_prediction_model.pkl")

from google.colab import files
files.download('Salary_prediction_model.pkl')

"""#Session_6"""

import joblib
joblib.dump(label_encoder,"label_encoder_sp.pkl")

from google.colab import files
files.download("label_encoder_sp.pkl")

!pip install streamlit

import numpy as np
import joblib
import streamlit as st
import pandas as pd

model=joblib.load("Salary_prediction_model.pkl")
encoder=joblib.load("label_encoder_sp.pkl")

st.title("Salary Prediction App")

Age	= st.number_input("Age",)
Gender = st.selectbox("Gender",encoder['Gender'].classes_)
Education_Level	= st.selectbox("Education_Level",encoder['Education Level'].classes_)
Job_Title	= st.selectbox("Job_Title",encoder['Job Title'].classes_)
Years_of_Experience = st.number_input("Years_of_Experience",0.0,40.0,2.0)

df = pd.DataFrame({
    "Age": [Age],
    "Gender": [Gender],
    "Education_Level": [Education_Level],
    "Job_Title": [Job_Title],
    "Years_of_Experience": [Years_of_Experience]
})

if st.button("Predict Salary"):
    for col in encoder:
        df[col] = encoder[col].transform(df[col])

    prediction = model.predict(df)
    st.success(f"Predicted Salary: {prediction[0]:,.2f}")
    st.success(f"Result: {result[0]}")